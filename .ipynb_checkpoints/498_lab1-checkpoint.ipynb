{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. lossFunction.function : function implementing the loss, (Z - y) ** 2.\n",
    "#    Provide a file lossFunction.py with \"function\" implemented within it\n",
    "\n",
    "# 2. Create the tensorflow computation graph. The graph outputs \n",
    "# the loss function that was described above. The function you\n",
    "# write must evaluate (a(X^t*X) + b^t*X - y) ** 2\n",
    "def  lossFunction(a,X,b,y):\n",
    "    X_t = tf.transpose(X)\n",
    "    b_t = tf.transpose(b)\n",
    "    adder1 = tf.multiply(a, tf.matmul(X_t,X))\n",
    "    # adder1 = a*(X_t@X)\n",
    "    adder2 = tf.matmul(b_t,X)\n",
    "    loss = (adder1+adder2-y)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X of correct shape has been returned\n",
      "Loss has shape [1, 1]\n",
      "Iteration 0, loss = 30.588106\n",
      "Iteration 1, loss = 30.203991\n",
      "Iteration 2, loss = 29.821474\n",
      "Iteration 3, loss = 29.440592\n",
      "Iteration 4, loss = 29.061388\n",
      "Iteration 5, loss = 28.683916\n",
      "Iteration 6, loss = 28.308212\n",
      "Iteration 7, loss = 27.934328\n",
      "Iteration 8, loss = 27.562290\n",
      "Iteration 9, loss = 27.192148\n",
      "Iteration 10, loss = 26.823956\n",
      "Iteration 11, loss = 26.457745\n",
      "Iteration 12, loss = 26.093571\n",
      "Iteration 13, loss = 25.731449\n",
      "Iteration 14, loss = 25.371433\n",
      "Iteration 15, loss = 25.013582\n",
      "Iteration 16, loss = 24.657885\n",
      "Iteration 17, loss = 24.304441\n",
      "Iteration 18, loss = 23.953224\n",
      "Iteration 19, loss = 23.604332\n",
      "Iteration 20, loss = 23.257759\n",
      "Iteration 21, loss = 22.913549\n",
      "Iteration 22, loss = 22.571724\n",
      "Iteration 23, loss = 22.232359\n",
      "Iteration 24, loss = 21.895441\n",
      "Iteration 25, loss = 21.561007\n",
      "Iteration 26, loss = 21.229101\n",
      "Iteration 27, loss = 20.899761\n",
      "Iteration 28, loss = 20.572971\n",
      "Iteration 29, loss = 20.248808\n",
      "Iteration 30, loss = 19.927242\n",
      "Iteration 31, loss = 19.608353\n",
      "Iteration 32, loss = 19.292145\n",
      "Iteration 33, loss = 18.978642\n",
      "Iteration 34, loss = 18.667835\n",
      "Iteration 35, loss = 18.359793\n",
      "Iteration 36, loss = 18.054491\n",
      "Iteration 37, loss = 17.751984\n",
      "Iteration 38, loss = 17.452261\n",
      "Iteration 39, loss = 17.155359\n",
      "Iteration 40, loss = 16.861280\n",
      "Iteration 41, loss = 16.570038\n",
      "Iteration 42, loss = 16.281670\n",
      "Iteration 43, loss = 15.996155\n",
      "Iteration 44, loss = 15.713536\n",
      "Iteration 45, loss = 15.433803\n",
      "Iteration 46, loss = 15.156983\n",
      "Iteration 47, loss = 14.883074\n",
      "Iteration 48, loss = 14.612084\n",
      "Iteration 49, loss = 14.344039\n",
      "Iteration 50, loss = 14.078920\n",
      "Iteration 51, loss = 13.816755\n",
      "Iteration 52, loss = 13.557547\n",
      "Iteration 53, loss = 13.301279\n",
      "Iteration 54, loss = 13.047980\n",
      "Iteration 55, loss = 12.797642\n",
      "Iteration 56, loss = 12.550269\n",
      "Iteration 57, loss = 12.305873\n",
      "Iteration 58, loss = 12.064442\n",
      "Iteration 59, loss = 11.825994\n",
      "Iteration 60, loss = 11.590510\n",
      "Iteration 61, loss = 11.357985\n",
      "Iteration 62, loss = 11.128441\n",
      "Iteration 63, loss = 10.901856\n",
      "Iteration 64, loss = 10.678245\n",
      "Iteration 65, loss = 10.457592\n",
      "Iteration 66, loss = 10.239889\n",
      "Iteration 67, loss = 10.025146\n",
      "Iteration 68, loss = 9.813349\n",
      "Iteration 69, loss = 9.604481\n",
      "Iteration 70, loss = 9.398549\n",
      "Iteration 71, loss = 9.195549\n",
      "Iteration 72, loss = 8.995469\n",
      "Iteration 73, loss = 8.798291\n",
      "Iteration 74, loss = 8.604014\n",
      "Iteration 75, loss = 8.412637\n",
      "Iteration 76, loss = 8.224128\n",
      "Iteration 77, loss = 8.038490\n",
      "Iteration 78, loss = 7.855710\n",
      "Iteration 79, loss = 7.675787\n",
      "Iteration 80, loss = 7.498686\n",
      "Iteration 81, loss = 7.324404\n",
      "Iteration 82, loss = 7.152932\n",
      "Iteration 83, loss = 6.984252\n",
      "Iteration 84, loss = 6.818352\n",
      "Iteration 85, loss = 6.655209\n",
      "Iteration 86, loss = 6.494813\n",
      "Iteration 87, loss = 6.337139\n",
      "Iteration 88, loss = 6.182178\n",
      "Iteration 89, loss = 6.029910\n",
      "Iteration 90, loss = 5.880321\n",
      "Iteration 91, loss = 5.733383\n",
      "Iteration 92, loss = 5.589088\n",
      "Iteration 93, loss = 5.447407\n",
      "Iteration 94, loss = 5.308323\n",
      "Iteration 95, loss = 5.171814\n",
      "Iteration 96, loss = 5.037862\n",
      "Iteration 97, loss = 4.906446\n",
      "Iteration 98, loss = 4.777545\n",
      "Iteration 99, loss = 4.651129\n",
      "Iteration 100, loss = 4.527180\n",
      "Iteration 101, loss = 4.405677\n",
      "Iteration 102, loss = 4.286597\n",
      "Iteration 103, loss = 4.169915\n",
      "Iteration 104, loss = 4.055604\n",
      "Iteration 105, loss = 3.943647\n",
      "Iteration 106, loss = 3.834011\n",
      "Iteration 107, loss = 3.726672\n",
      "Iteration 108, loss = 3.621601\n",
      "Iteration 109, loss = 3.518784\n",
      "Iteration 110, loss = 3.418184\n",
      "Iteration 111, loss = 3.319787\n",
      "Iteration 112, loss = 3.223556\n",
      "Iteration 113, loss = 3.129462\n",
      "Iteration 114, loss = 3.037481\n",
      "Iteration 115, loss = 2.947591\n",
      "Iteration 116, loss = 2.859761\n",
      "Iteration 117, loss = 2.773962\n",
      "Iteration 118, loss = 2.690164\n",
      "Iteration 119, loss = 2.608341\n",
      "Iteration 120, loss = 2.528471\n",
      "Iteration 121, loss = 2.450511\n",
      "Iteration 122, loss = 2.374456\n",
      "Iteration 123, loss = 2.300257\n",
      "Iteration 124, loss = 2.227887\n",
      "Iteration 125, loss = 2.157327\n",
      "Iteration 126, loss = 2.088542\n",
      "Iteration 127, loss = 2.021506\n",
      "Iteration 128, loss = 1.956193\n",
      "Iteration 129, loss = 1.892566\n",
      "Iteration 130, loss = 1.830600\n",
      "Iteration 131, loss = 1.770272\n",
      "Iteration 132, loss = 1.711544\n",
      "Iteration 133, loss = 1.654393\n",
      "Iteration 134, loss = 1.598791\n",
      "Iteration 135, loss = 1.544708\n",
      "Iteration 136, loss = 1.492114\n",
      "Iteration 137, loss = 1.440982\n",
      "Iteration 138, loss = 1.391285\n",
      "Iteration 139, loss = 1.342995\n",
      "Iteration 140, loss = 1.296081\n",
      "Iteration 141, loss = 1.250517\n",
      "Iteration 142, loss = 1.206273\n",
      "Iteration 143, loss = 1.163328\n",
      "Iteration 144, loss = 1.121651\n",
      "Iteration 145, loss = 1.081213\n",
      "Iteration 146, loss = 1.041987\n",
      "Iteration 147, loss = 1.003952\n",
      "Iteration 148, loss = 0.967076\n",
      "Iteration 149, loss = 0.931332\n",
      "Iteration 150, loss = 0.896700\n",
      "Iteration 151, loss = 0.863152\n",
      "Iteration 152, loss = 0.830658\n",
      "Iteration 153, loss = 0.799200\n",
      "Iteration 154, loss = 0.768747\n",
      "Iteration 155, loss = 0.739280\n",
      "Iteration 156, loss = 0.710772\n",
      "Iteration 157, loss = 0.683197\n",
      "Iteration 158, loss = 0.656533\n",
      "Iteration 159, loss = 0.630763\n",
      "Iteration 160, loss = 0.605854\n",
      "Iteration 161, loss = 0.581787\n",
      "Iteration 162, loss = 0.558544\n",
      "Iteration 163, loss = 0.536097\n",
      "Iteration 164, loss = 0.514426\n",
      "Iteration 165, loss = 0.493511\n",
      "Iteration 166, loss = 0.473333\n",
      "Iteration 167, loss = 0.453867\n",
      "Iteration 168, loss = 0.435096\n",
      "Iteration 169, loss = 0.416998\n",
      "Iteration 170, loss = 0.399557\n",
      "Iteration 171, loss = 0.382750\n",
      "Iteration 172, loss = 0.366559\n",
      "Iteration 173, loss = 0.350968\n",
      "Iteration 174, loss = 0.335953\n",
      "Iteration 175, loss = 0.321506\n",
      "Iteration 176, loss = 0.307601\n",
      "Iteration 177, loss = 0.294224\n",
      "Iteration 178, loss = 0.281362\n",
      "Iteration 179, loss = 0.268993\n",
      "Iteration 180, loss = 0.257105\n",
      "Iteration 181, loss = 0.245681\n",
      "Iteration 182, loss = 0.234705\n",
      "Iteration 183, loss = 0.224165\n",
      "Iteration 184, loss = 0.214046\n",
      "Iteration 185, loss = 0.204331\n",
      "Iteration 186, loss = 0.195009\n",
      "Iteration 187, loss = 0.186064\n",
      "Iteration 188, loss = 0.177487\n",
      "Iteration 189, loss = 0.169262\n",
      "Iteration 190, loss = 0.161378\n",
      "Iteration 191, loss = 0.153823\n",
      "Iteration 192, loss = 0.146585\n",
      "Iteration 193, loss = 0.139652\n",
      "Iteration 194, loss = 0.133013\n",
      "Iteration 195, loss = 0.126659\n",
      "Iteration 196, loss = 0.120578\n",
      "Iteration 197, loss = 0.114758\n",
      "Iteration 198, loss = 0.109193\n",
      "Iteration 199, loss = 0.103872\n",
      "Iteration 200, loss = 0.098785\n",
      "Iteration 201, loss = 0.093923\n",
      "Iteration 202, loss = 0.089278\n",
      "Iteration 203, loss = 0.084841\n",
      "Iteration 204, loss = 0.080605\n",
      "Iteration 205, loss = 0.076561\n",
      "Iteration 206, loss = 0.072701\n",
      "Iteration 207, loss = 0.069017\n",
      "Iteration 208, loss = 0.065505\n",
      "Iteration 209, loss = 0.062155\n",
      "Iteration 210, loss = 0.058961\n",
      "Iteration 211, loss = 0.055917\n",
      "Iteration 212, loss = 0.053017\n",
      "Iteration 213, loss = 0.050255\n",
      "Iteration 214, loss = 0.047624\n",
      "Iteration 215, loss = 0.045120\n",
      "Iteration 216, loss = 0.042736\n",
      "Iteration 217, loss = 0.040469\n",
      "Iteration 218, loss = 0.038311\n",
      "Iteration 219, loss = 0.036259\n",
      "Iteration 220, loss = 0.034309\n",
      "Iteration 221, loss = 0.032454\n",
      "Iteration 222, loss = 0.030693\n",
      "Iteration 223, loss = 0.029020\n",
      "Iteration 224, loss = 0.027431\n",
      "Iteration 225, loss = 0.025922\n",
      "Iteration 226, loss = 0.024490\n",
      "Iteration 227, loss = 0.023130\n",
      "Iteration 228, loss = 0.021842\n",
      "Iteration 229, loss = 0.020619\n",
      "Iteration 230, loss = 0.019460\n",
      "Iteration 231, loss = 0.018360\n",
      "Iteration 232, loss = 0.017319\n",
      "Iteration 233, loss = 0.016333\n",
      "Iteration 234, loss = 0.015398\n",
      "Iteration 235, loss = 0.014514\n",
      "Iteration 236, loss = 0.013677\n",
      "Iteration 237, loss = 0.012885\n",
      "Iteration 238, loss = 0.012135\n",
      "Iteration 239, loss = 0.011426\n",
      "Iteration 240, loss = 0.010755\n",
      "Iteration 241, loss = 0.010122\n",
      "Iteration 242, loss = 0.009523\n",
      "Iteration 243, loss = 0.008957\n",
      "Iteration 244, loss = 0.008422\n",
      "Iteration 245, loss = 0.007917\n",
      "Iteration 246, loss = 0.007441\n",
      "Iteration 247, loss = 0.006992\n",
      "Iteration 248, loss = 0.006568\n",
      "Iteration 249, loss = 0.006168\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from functools import reduce\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf \n",
    "# import initializeX\n",
    "# import lossFunction\n",
    "# import optimizerFunction\n",
    "# import computeLoss\n",
    "# import trainStep\n",
    "# import plotFunction\n",
    "\n",
    "# # Preliminary setup, do not modify\n",
    "# if len(sys.argv) > 1:\n",
    "#     random.seed(int(sys.argv[1]));\n",
    "#     np.random.seed(int(sys.argv[1]));\n",
    "# else:\n",
    "random.seed(int(1));\n",
    "np.random.seed(int(1));\n",
    "\n",
    "def shape(V):\n",
    "    return list(map(int, list(V.shape)));\n",
    "\n",
    "total = lambda shape : reduce(lambda x, y : x * y, shape, 1);\n",
    "assert(total([3,4,1]) == 12);\n",
    "# Preliminary setup over\n",
    "\n",
    "\"\"\"\n",
    "In this experiment, we are solving for a value of X that results in a(X^t*X) + b^t*X = y (* represents \n",
    "matrix multiply, and ^t represents transpose). Here X, b are a column-vectors of shape [4,1], a\n",
    "and y are scalar constants. We will use gradient descend to perform this task. \n",
    "We setup the problem as follows: we compute Z = a*(X^t*X) + b^t*X, and then\n",
    "the quantity (Z - y)**2. This quantity is called the 'loss' in our problem setting. Note that loss\n",
    "is higher when Z deviates from y, and lower otherwise. We will use tensorflow to minimize the value of the loss by\n",
    "adjusting the value of X 'appropriately'. This is done by repetitively computing the\n",
    "gradient of the loss with respect to X, and adjusting X using this gradient.\n",
    "\n",
    "You will implement the following functions for this problem\n",
    "1. initializeX.function : function returning the tensorflow variable X initialized to a random value.\n",
    "   Provide a file initializeX.py with \"function\" implemented within it\n",
    "\n",
    "2. lossFunction.function : function implementing the loss, (Z - y) ** 2.\n",
    "   Provide a file lossFunction.py with \"function\" implemented within it\n",
    "\n",
    "3. optimizerFunction.function : function implementing the training step\n",
    "   Provide a file optimizerFunction.py with \"function\" implemented within it\n",
    "\n",
    "4. computeLoss.function : function that provides a printable value of the loss. By printable, what we mean\n",
    "   is that the value of loss is visible on using python print(). Using print directly on tensorflow variables\n",
    "   or constants doesn't show their value.\n",
    "   Provide a file computeLoss.py with \"function\" implemented within it\n",
    "\n",
    "5. trainStep.function : function that implements the training step\n",
    "   Provide a file trainStep.py with \"function\" implemented within it\n",
    "\n",
    "\"\"\"\n",
    "tf.disable_eager_execution()\n",
    "#1. Create the constant a\n",
    "a = tf.constant(10, dtype=tf.float32);\n",
    "\n",
    "#2. Create the variable X. Here, recommended that you initialize X \n",
    "# from a numpy array with random numbers selected from between 0 and 1.\n",
    "# np.seed使得每次X产生的随机数固定\n",
    "X = tf.Variable(np.random.rand(4,1),dtype=tf.float32)\n",
    "\n",
    "shapeOfX = shape(X);\n",
    "if not ((len(shapeOfX) == 2) and (shapeOfX[0] == 4) and (shapeOfX[1] == 1)):\n",
    "    raise ValueError(\"Variable X doesn't have the correct shape\");\n",
    "else:\n",
    "    print(\"X of correct shape has been returned\");\n",
    "\n",
    "#3. Create the constant b\n",
    "b = tf.constant(np.arange(4).reshape((4,1)), dtype=tf.float32);\n",
    "\n",
    "#3. Create the constant y\n",
    "y = tf.constant(15, dtype=tf.float32);\n",
    "\n",
    "#4. Create the tensorflow computation graph. The graph outputs \n",
    "# the loss function that was described above. The function you\n",
    "# write must evaluate (a(X^t*X) + b^t*X - y) ** 2\n",
    "loss = lossFunction(a, X, b, y);\n",
    "\n",
    "shapeOfLoss = shape(loss);\n",
    "print(\"Loss has shape\", shapeOfLoss);\n",
    "assert(total(shapeOfLoss) == 1), \"Loss is not a scalar!\";\n",
    "\n",
    "#5. Create the AdamOptimizer. Optimizers add additional nodes \n",
    "# in the tensorflow graph to compute gradients as well as apply them to the variables involved.\n",
    "# This could be manually performed using tf.gradients etc, but it is a process that is repeated\n",
    "# over and over in all Deep Neural Networks, so the optimizers hide all the gory details.\n",
    "# In addition, optimizers do things other than calculate simple gradients in order to ensure\n",
    "# that convergence happens quickly. \"Executing\" the optimizer inside a tf.Session hence implements\n",
    "# 1) computation of gradients with respect to all the variables in the graph, and 2) adjusting the value\n",
    "# of the variables using these gradients\n",
    "lr=1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "#6. Launch the training loop. We want to track the loss function over the training iterations\n",
    "# We will launch 250 training iterations\n",
    "session = tf.Session();\n",
    "session.run(tf.global_variables_initializer());\n",
    "\n",
    "lossValues = [];\n",
    "\n",
    "for i in range(250):\n",
    "    # 7. Implement a function that provides the printable value of loss\n",
    "    lossValue = loss.eval(session=session)[0][0]\n",
    "\n",
    "    # 8. Implement a function that performs loss minimization\n",
    "    session.run(optimizer)\n",
    "\n",
    "    # 9. Print out loss\n",
    "    print(\"Iteration %d, loss = %f\"%(i, lossValue));\n",
    "\n",
    "    lossValues.append(lossValue);\n",
    "\n",
    "### Please note that the following is not for demo, but only for the report (hence, currently commented out)\n",
    "# You may prepare the plot using another tool like Excel, but this is the recommended way.\n",
    "# # 10. Finally, add a function to plot the loss value across training steps\n",
    "# # Please refer to the python library, pyplot https://matplotlib.org/users/pyplot_tutorial.html\n",
    "# plotFunction.function(lossValues);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c is the view of  a\n",
    "a = np.array([1,2])\n",
    "c = a\n",
    "c[0] = 2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Flatten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c362beb57c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-c362beb57c7d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Build and compile the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         self.discriminator.compile(loss='binary_crossentropy', \n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-c362beb57c7d>\u001b[0m in \u001b[0;36mbuild_discriminator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Flatten' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28 \n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)\n",
    "\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        model =  keras.Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"gan/images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=32, save_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
